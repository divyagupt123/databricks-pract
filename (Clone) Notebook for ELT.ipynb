{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91cc38db-cf38-47ce-b179-2a03addc8eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Extract raw CSV\n",
    "# raw_df = spark.read.option(\"header\", True).csv(\"abfss://hgcontainer@hgstoragenew.dfs.core.windows.net/landing/customer_churn_data.csv\")\n",
    "\n",
    "# # display(raw_df)\n",
    "\n",
    "# # Load to Bronze\n",
    "# raw_df.write.format(\"delta\").mode(\"overwrite\").option(\"path\",\"abfss://hgcontainer@hgstoragenew.dfs.core.windows.net/bronze/telecom_churn_data\").saveAsTable(\"dev.bronze.telecom_churn_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89bfaf9a-630d-474e-afbc-ad77247b34a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Landing to Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee3755fe-6d63-445f-a724-cbf78403f0af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, current_timestamp\n",
    "from pyspark.sql import Row\n",
    "import os\n",
    "# Define paths and log table\n",
    "landing_path = \"abfss://hgcontainer@hgstoragenew.dfs.core.windows.net/landing/\"\n",
    "bronze_path = \"abfss://hgcontainer@hgstoragenew.dfs.core.windows.net/bronze/telecom_churn_data/\"\n",
    "log_table = \"dev.bronze.file_ingestion_log\"\n",
    "\n",
    "# Step 1: Create ingestion log table if not exists\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dev.bronze.file_ingestion_log (\n",
    "    file_name STRING,\n",
    "    file_path STRING,\n",
    "    load_timestamp TIMESTAMP,\n",
    "    status STRING\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION 'abfss://hgcontainer@hgstoragenew.dfs.core.windows.net/bronze/file_ingestion_log/'\n",
    "\"\"\")\n",
    "\n",
    "# Step 2: List all .csv files in landing zone\n",
    "all_files = [f.path for f in dbutils.fs.ls(landing_path) if f.path.endswith(\".csv\")]\n",
    "\n",
    "print(all_files)\n",
    "\n",
    "# Step 3: Get already processed files\n",
    "processed_files = (\n",
    "    spark.table(log_table)\n",
    "    .select(\"file_path\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "print(processed_files)\n",
    "# Step 4: Filter new/unprocessed files\n",
    "new_files = [f for f in all_files if f not in processed_files]\n",
    "print(new_files)\n",
    "# Step 5: Ingest new files and save as Parquet, then log\n",
    "for file_path in new_files:\n",
    "    try:\n",
    "        # Read CSV from landing\n",
    "        df = spark.read.option(\"header\", True).csv(file_path)\n",
    "        df = df.withColumn(\"source_file\", input_file_name())\n",
    "            \n",
    "        # Write as Parquet to bronze path using file_name folder to separate\n",
    "        file_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
    "        df.write.mode(\"overwrite\").parquet(f\"{bronze_path}/{file_name}\")\n",
    "\n",
    "        # Log successful ingestion\n",
    "        log_df = spark.createDataFrame([\n",
    "            Row(file_name=os.path.basename(file_path), file_path=file_path, status=\"SUCCESS\")\n",
    "        ]).withColumn(\"load_timestamp\", current_timestamp())\n",
    "\n",
    "        log_df.write.mode(\"append\").saveAsTable(log_table)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {file_path}: {e}\")\n",
    "\n",
    "        # Log failure\n",
    "        log_df = spark.createDataFrame([\n",
    "            Row(file_name=os.path.basename(file_path), file_path=file_path, status=\"FAILED\")\n",
    "        ]).withColumn(\"load_timestamp\", current_timestamp())\n",
    "\n",
    "        log_df.write.mode(\"append\").saveAsTable(log_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a10c946-bf4e-4750-8929-7575dc63bb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Bronze to silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf26ac8-6da7-4489-b9df-6514dbae28b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sha2, current_timestamp\n",
    "from pyspark.sql import Row\n",
    "import os\n",
    "\n",
    "bronze_base_path = \"abfss://hgcontainer@hgstoragenew.dfs.core.windows.net/bronze/telecom_churn_data/\"\n",
    "#silver_path = \"abfss://hgcontainer@hgstoragenew.dfs.core.windows.net/silver/\"\n",
    "\n",
    "\n",
    "\n",
    "# Create log table if not exists\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dev.silver.bronze_to_silver_log (\n",
    "    file_name STRING,\n",
    "    file_path STRING,\n",
    "    processed_at TIMESTAMP,\n",
    "    status STRING\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION 'abfss://hgcontainer@hgstoragenew.dfs.core.windows.net/silver/bronze_to_silver_log/'\n",
    "\"\"\")\n",
    "\n",
    "# List all folders in Bronze\n",
    "bronze_folders = [f.path for f in dbutils.fs.ls(bronze_base_path) if f.isDir()]\n",
    "print( bronze_folders)\n",
    "# Get already processed\n",
    "processed_paths = (\n",
    "    spark.table(\"dev.silver.bronze_to_silver_log\")\n",
    "    .select(\"file_path\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Filter only unprocessed folders\n",
    "new_folders = [f for f in bronze_folders if f not in processed_paths]\n",
    "\n",
    "# Process each new folder\n",
    "for file_path in new_folders:\n",
    "    try:\n",
    "        df = spark.read.option(\"header\", True).parquet(file_path)\n",
    "\n",
    "        df_filled = df.fillna({\n",
    "            \"Age\": 0,\n",
    "            \"Gender\": \"Unknown\",\n",
    "            \"Tenure\": 0,\n",
    "            \"MonthlyCharges\": 0.0,\n",
    "            \"ContractType\": \"Unknown\",\n",
    "            \"InternetService\": \"Unknown\",\n",
    "            \"TotalCharges\": 0.0,\n",
    "            \"TechSupport\": \"Unknown\",\n",
    "            \"Churn\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        df_anonymized = df_filled.withColumn(\"CustomerID\", sha2(col(\"CustomerID\"), 256)) \\\n",
    "                                 .withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "        df_cleaned = df_anonymized.filter(\n",
    "            (col(\"MonthlyCharges\") >= 0) & (col(\"TotalCharges\") >= 0)\n",
    "        )\n",
    "\n",
    "        df_cleaned.write.mode(\"append\").format(\"delta\").option(\"path\", \"abfss://hgcontainer@hgstoragenew.dfs.core.windows.net/silver/telecom_churn_data\").saveAsTable(\"dev.silver.customer_data\")\n",
    "\n",
    "        log_df = spark.createDataFrame([\n",
    "    Row(file_name=os.path.basename(file_path.rstrip(\"/\")), file_path=file_path, status=\"SUCCESS\")\n",
    "]).withColumn(\"processed_at\", current_timestamp())\n",
    "        log_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"dev.silver.bronze_to_silver_log\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        log_df = spark.createDataFrame([\n",
    "    Row(file_name=os.path.basename(file_path.rstrip(\"/\")), file_path=file_path, status=\"FAILED\")\n",
    "]).withColumn(\"processed_at\", current_timestamp())\n",
    "        log_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"dev.silver.bronze_to_silver_log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b2a67c-e523-4c98-956e-979d9c31cb2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql \n",
    "# drop table dev.silver.customer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a06231f6-06e5-4d64-b566-4b2fe8284f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fabf2b9-933e-4f17-b28f-170098c5e13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd_count = rdd.map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y)\n",
    "rdd_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f45adea-fe82-4e9d-b593-490d8e0dd1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [('James','Smith','M',30,'UK'),\n",
    "  ('Anna','Rose','F',41,'DBA'),\n",
    "  ('Robert','Williams','M',62,'DEL'), \n",
    "]\n",
    "\n",
    "map_country = {'UK': 'USA', 'DBA': 'Canada', 'DEL': 'Germany'} \n",
    "broadcast_map = sc.broadcast(map_country)   \n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\",\"Country\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3c62eb-8715-41c2-b0bf-5a0ca4535dbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,concat_ws,concat\n",
    "df1 = df.withColumn(\"name\",concat(df.firstname,df.lastname)).show()\n",
    "df2 = df.withColumn(\"name\", concat_ws(\" \",df.firstname, df.lastname))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55c9eec-e4ba-4ad7-95f5-56e06ed8c38f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.rdd.map(lambda x: (x[\"name\"], x[\"gender\"], x[\"salary\"],broadcast_map.value.get(x[\"Country\"], \"Unknown\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f657ba-6d2e-4524-b80b-c74381275122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for element in rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c2cc800-62d4-48c0-aad5-470e9f5a32be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd091f0-2fb9-4ef6-a88e-07607980a4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "df = df.withColumn(\"languagesAtSchool\",explode(col(\"languagesAtSchool\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca95798-7a44-41de-8860-c46b76f4a28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df2 = df.withColumn(\"fname\",split(col(\"name\"),\",\")[0]) .withColumn(\"mname\",split(col(\"name\"),\",\")[1]).withColumn(\"lname\",split(col(\"name\"),\",\")[2]).drop(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5796e834-151a-4d49-a90b-ed3ae1560963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkByExamples\").getOrCreate()\n",
    "\n",
    "# Sample Data with two lists\n",
    "names = [\"Ricky\", \"Bunny\", \"Coco\"]\n",
    "ages = [10, 15, 20]\n",
    "\n",
    "df = spark.createDataFrame(zip(names, ages),[\"names\", \"ages\"])\n",
    "df.withColumn(\"ages\", col(\"ages\").cast(\"String\")).show()\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd502e9e-2037-48c3-893c-9556420802cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "\n",
    "# Create DataFrame\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6915daa9-6c1a-4adf-8949-a7fa73b5e6f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feae8a6a-7353-4dcd-9ecd-a225f45579ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, replace\n",
    "columns = df.columns\n",
    "df1 = df.select([col(c).alias(f\"{c}_add\") for c in columns])\n",
    "df1.show()\n",
    "columns1 = df1.columns\n",
    "print(columns1)\n",
    "df2 = df1.select(*[col(c).alias(c.replace(\"_add\",\"\")) for c in columns1])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a75280c4-7267-4401-b937-14d3e89d5f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"event_status_example\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"success\", \"2024-01-01\"),\n",
    "    (\"success\", \"2024-01-03\"),\n",
    "    (\"failed\", \"2024-01-02\"),\n",
    "    (\"in_progress\", \"2024-01-04\"),\n",
    "    (\"success\", \"2024-01-05\"),\n",
    "    (\"failed\", \"2024-01-06\"),\n",
    "    (\"in_progress\", \"2024-01-07\"),\n",
    "    (\"failed\", \"2024-01-08\"),\n",
    "    (\"success\", \"2024-01-09\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"event_status\", \"event_date\"])\n",
    "\n",
    "# Show the base data\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f481d9-c615-4af3-aeea-57ac1a9b3e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "#window_spec = window.partitionBy(\"event_status\").orderBy((\"event_date\").desc())\n",
    "window_spec = Window.partitionBy(\"event_status\").orderBy(col(\"event_date\"))\n",
    "df_with_lag = df.withColumn(\"start_date\", lead(\"event_date\").over(window_spec))\n",
    "df_with_lag.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1992078965013544,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Notebook for ELT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
